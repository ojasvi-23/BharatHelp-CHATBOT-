# -*- coding: utf-8 -*-
"""Integrating OpenAI GPT-4 for Q&A

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e8v5W6wkjcMaHWdvqkyY3XgQfKSlR0db
"""

# Step 4: Integrating OpenAI GPT-4 for Q&A

# Initialize the ChatOpenAI model (GPT-4).
# We'll use 'gpt-4' or 'gpt-4-turbo-preview' for better performance and multilingual capabilities.
# If you don't have access to GPT-4, you can use 'gpt-3.5-turbo' instead.
llm = ChatOpenAI(model_name="gpt-4-turbo-preview", temperature=0.7)
print(f"ChatOpenAI model '{llm.model_name}' initialized.")

# Define a custom prompt template for the LLM.
# This prompt guides the LLM on how to use the provided context and answer the user's question.
# It also encourages multilingual responses.
prompt_template = """
You are BharatHelp, a friendly and helpful AI assistant.
You specialize in providing information about BharatHelp's policies and products.
Answer the user's question based *only* on the provided context.
If the answer is not found in the context, politely state that you don't have information on that topic.
Maintain a helpful and polite tone.
If the user asks in Hindi or Hinglish, try to respond in the same language. If the user asks in English, respond in English.

Context:
{context}

Question: {question}

Answer:
"""
PROMPT = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"]
)
print("Prompt template defined.")

# Create a RetrievalQA chain.
# This chain combines the retriever (to get relevant documents) and the LLM (to generate answers).
# It fetches relevant docs, passes them to the LLM along with the query, and gets an answer.
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff", # 'stuff' means it stuffs all retrieved documents into the prompt.
    retriever=retriever,
    return_source_documents=True, # Useful for debugging and showing where the info came from
    chain_type_kwargs={"prompt": PROMPT}
)
print("RetrievalQA chain created.")

# Test the QA chain with various queries (English, Hindi, Hinglish)
print("\n--- Testing QA Chain ---")

# English query
query_en = "What are the different plans for BharatMobile?"
print(f"\nUser (English): {query_en}")
result_en = qa_chain({"query": query_en})
print(f"BharatHelp: {result_en['result']}")
# print(f"Source Documents: {[doc.metadata for doc in result_en['source_documents']]}") # Uncomment to see source docs

# Hindi query
query_hi = "भारतकनेक्ट के प्लान क्या हैं?"
print(f"\nUser (Hindi): {query_hi}")
result_hi = qa_chain({"query": query_hi})
print(f"BharatHelp: {result_hi['result']}")

# Hinglish query
query_hinglish = "Mujhe BharatConnect ka technical support number chahiye."
print(f"\nUser (Hinglish): {query_hinglish}")
result_hinglish = qa_chain({"query": query_hinglish})
print(f"BharatHelp: {result_hinglish['result']}")

# Query for information not in the knowledge base
query_unknown = "What is the capital of France?"
print(f"\nUser (English): {query_unknown}")
result_unknown = qa_chain({"query": query_unknown})
print(f"BharatHelp: {result_unknown['result']}")

print("\nQA chain testing complete.")